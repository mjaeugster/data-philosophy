---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("ggplot2")
library("tibble")
library("purrr")
library("dplyr")
```

Baseline conversion rate:
```{r}
cr_a <- 0.1
```

We expect an uplift in % of
```{r}
up <- 20
```
and therefore a conversion rate of:
```{r}
cr_b <- cr_a + cr_a * up / 100
cr_b
```

Signifance and power are defined as:
```{r}
sig <- 0.05
power <- 0.8
```

With this definition, the required number of observations per group is:
```{r}
n_group <- ceiling(power.prop.test(p1 = cr_a, p2 = cr_b, sig.level = sig, power = power)$n)
n_group
```

If we run the test with the required number of observations per group and there
really is an uplift of 30% we get a significant result with
* with the probability of rejecting the null hypothesis, given that the null hypothesis is true


In testing of hypothesis studies, the objective of sample size calculation is to achieve a desired power for detecting a clinically or scientifically meaningful difference at a prefixed level of significance.


## Simulating a conversion test

```{r}
test <- function(a, b, n) { 
  prop.test(c(a, b), c(n, n), conf.level = 1 - sig)$p.value
}

run <- function(b, n, cr_a, cr_b) {
  n <- ceiling(n / b)
  ns <- seq(b, by = b, length.out = n)
  sc_a <- cumsum(rbinom(n, b, cr_a))
  sc_b <- cumsum(rbinom(n, b, cr_b))
  
  ps <- mapply(test, sc_a, sc_b, ns)
  
  tibble(n = ns, ca = sc_a, cb = sc_b, p = ps)
}
```

Batch size and total number of observations per group:
```{r}
b <- 100
n <- n_group * 3
```

Difference between both versions:
```{r}
set.seed(1213)
e1 <- run(b, n, cr_a, cr_b)

ggplot(e1, aes(n, p)) + 
  geom_line() + 
  geom_hline(yintercept = sig, colour = "red", linetype = "dashed") +
  geom_vline(xintercept = n_group, colour = "red", linetype = "dashed") +
  ylim(c(0, 1))
```

No difference between both versions:
```{r}
r <- function(i) {
  set.seed(1000 + i)
  e <- run(b, n, cr_a, cr_a)
  
  c(seed = 1000+i, cnt = sum(e$p[1:floor(n_group / b)] < sig))
}

e2 <- lapply(1:1000, r)
e2 <- do.call(rbind, e2)

head(e2[order(e2[, "cnt"], decreasing = TRUE), ])
```

```{r}
set.seed(1563)
e <- run(b, n, cr_a, cr_a)
  
ggplot(e, aes(n, p)) + 
  geom_line() + 
  geom_hline(yintercept = sig, colour = "red", linetype = "dashed") +
  geom_vline(xintercept = n_group, colour = "red", linetype = "dashed") +
  ylim(c(0, 1))
```

This is an example that you 1) should not stop early and 2) do not talk 
about "it is 90% significant".

Why is this happening?

```{r}
e %>%
  gather(ca, cb, key = "version", value = "count") %>%
  ggplot(aes(n, count, colour = version)) +
  geom_line() +
  geom_vline(xintercept = n_group, colour = "red", linetype = "dashed")
```


## Power simulation

The power of any statistical test is the probability that it will reject a false null hypothesis; 
i.e., the probability of detecting an effect, given that the effect is really there.

```{r}
l1 <- rerun(100, run(b, n, cr_a, cr_b))
l1 <- bind_rows(l1, .id = "run")

ggplot(l1, aes(n, p, group = run)) +
  geom_line(colour = rgb(0, 0, 0, 0.2)) + 
  geom_vline(xintercept = n_group, colour = "red", linetype = "dashed") +
  geom_hline(yintercept = sig, colour = "red", linetype = "dashed")

l1 %>%
  filter(n > n_group) %>%
  group_by(run) %>%
  summarize(a = any(p > 0.05)) %>%
  select(a) %>%
  table() %>%
  prop.table() * 100
```


## Significance

The significance of a statistical test is the probability of rejecting a true null hypothesis.

```{r}
l2 <- rerun(100, run(b, n, cr_a, cr_a))
l2 <- bind_rows(l2, .id = "run")

ggplot(l2, aes(n, p, group = run)) +
  geom_line(colour = rgb(0, 0, 0, 0.2)) + 
  geom_vline(xintercept = n_group, colour = "red", linetype = "dashed") +
  geom_hline(yintercept = sig, colour = "red", linetype = "dashed")


l1 %>%
  filter(n == n_group) %>%
  group_by(run) %>%
  summarize(a = any(p < 0.05)) %>%
  select(a) %>%
  table() %>%
  prop.table() * 100

```

```{r}
s <- ceiling(n_group/100)
e <- ceiling(n_group * 3 / 100)

prop.table(table(colSums(l[s, , drop = FALSE] < 0.05) > 0)) * 100
prop.table(table(colSums(l[s:e, , drop = FALSE] < 0.05) > 0)) * 100
prop.table(table(colSums(l[, , drop = FALSE] < 0.05) > 0)) * 100
```



## Material:

- Optimizely is moving away from traditional, fixed horizon hypothesis testing to sequential
testing and replacing Type I error control with false discovery rate (FDR) control
- http://pages.optimizely.com/rs/optimizely/images/stats_engine_technical_paper.pdf